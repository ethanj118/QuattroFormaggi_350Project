{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_Blog.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2QYBpAWVarQ"
      },
      "source": [
        "# **Amazon Rekognition and Facial Expressions**\n",
        "\n",
        "## **Quattro Formaggi**\n",
        "\n",
        "##### Haozhou Huang, Luna Li, Charlie Mather, Ethan Jampel, Feiyang Xue\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUOzGadCX5X0"
      },
      "source": [
        "## **Intro**\n",
        "\n",
        "Facial recognition and machine learning technologies have a wide variety of uses, and have become more widespread in recent years. One of these uses is the recognition of facial expressions and emotions. Using facial expressions to recognize emotions is an especially interesting topic because there is debate on whether or not emotion recognition from expressions is universal. Matsumoto and Hwang (2011) [provide an excellent overview of this topic](https://www.apa.org/science/about/psa/2011/05/facial-expressions), and give great background information. They also explain how the ability to recognize emotions can be helpful in face-to-face interactions. Using machine learning and streams of data, algorithms can be trained to predict one's emotions based on a picture of their face and expression. This technology has widespread uses, and one of our group members has experience with it from an internship application. She was provided with a set of pictures of people making subtle facial expressions, and was asked to predict the emotion the facial expression represented. We were interested in seeing how Amazon Rekognition would do with this same task. Rekognition is able to recognize facial expressions and give its predictions as an output. This blog details our findings and compares them with what a human might respond. This task can point to whether or not recognition of emotion might be an area where facial recognition software can continue to grow, or if there are still major differences between the predictions of humans and the predictions of algorithms. The link between facial expressions and emotions may not be the same for everyone, as [this article](https://www.nature.com/articles/d41586-020-00507-5) by Douglas Heaven explains. In fact, Heaven makes the link between emotion recognition and machine learning to explain why it might not always be so easy.\n",
        "\n",
        "## **Hypothesis**\n",
        "\n",
        "While Rekognition is a model that has been trained with a huge amount of data and is [constantly learning more](https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html), recognition of emotions based on facial that are intentionally subtle may still be a problem. When the model is presented with a subtle expression, it may struggle to differentiate it from other emotions and may make errors. Humans can also make mistakes, and do not have access to the massive number of data points that Rekognition uses to train and predict. At the same time, humans may be able to recognize small nuances that Rekognition can't. Given the difficulty level of this task, we hypothesize that the results Rekognition comes up with will be significantly different than the answers our group comes up with. Furthermore, we predict that for at least one image, our prediction will be different from a Rekognition prediction that has a confidence level over 90. \n",
        "\n",
        "## **Infrastructure Overview**\n",
        "\n",
        "This project employs a number of AWS services, which are displayed in the architecture diagram below. While Rekognition is the machine learning model we are using, other AWS services are necessary to complete this project.\n",
        "\n",
        "![Architecture Diagram](https://raw.githubusercontent.com/ethanj118/QuattroFormaggi_350Project/main/Charts%20and%20Diagrams/Architecture%20diagram.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIa4jH-hANW6"
      },
      "source": [
        "The architecture diagram above details the process from data collection to analyzed output. \n",
        "\n",
        "**Data Collection**\n",
        "\n",
        "To collect the data for this project, we used an online video displaying illustrated faces making subtle facial expressions. An example face can be found at [this link](https://github.com/ethanj118/QuattroFormaggi_350Project/blob/main/Raw%20Data/IMG_1042.jpg). We pulled images from this video by taking screenshots (step three in the diagram) and cropping them, which gave us a total of 20 data points. Once cropped, these images were ready for use with Rekognition. These images are similar to what our group member saw in the test she took as part of an internship application. They clearly show a face, so Rekognition can recognize the face and predict its expression. After this, we were ready to use or first AWS service.\n",
        "\n",
        "**Storage using S3 Buckets**\n",
        "\n",
        "We can upload these images to S3 buckets. [S3 buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html) are a simple storage method where files can be placed for storage and/or for later use with an AWS ML service. Below is an photo of the bucket interface using an example bucket from a previous project.\n",
        " ![Bucket Example](https://raw.githubusercontent.com/ethanj118/QuattroFormaggi_350Project/main/Blog/Blog%20Images/Bucket_example.png)\n",
        "\n",
        "In order to secure our buckets, we can use another AWS service, **Identity and Access Management**, the red icon on the diagram. IAM allows us to set permissions on our buckets so we can ensure that all group members have access, but the buckets and our data are not public. In addition to securing our bucket, IAM also allows us to access Rekognition. On our SageMaker notebook, we need to attach a permission policy for Rekognition. We can do this by clicking our notebook instance, clicking the link in the \"Permissions and ecryption\" section, and clicking attach policy and searching for the RekognitionFullAccess policy. Once it is attached, it will show up in the policy list, like in the screenshot below.\n",
        "\n",
        "![IAM Example](https://raw.githubusercontent.com/ethanj118/QuattroFormaggi_350Project/main/Blog/Blog%20Images/IAM_example.png)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0DLQ746ImNi"
      },
      "source": [
        "**SageMaker and Rekognition**\n",
        "\n",
        "After uploading our data to a bucket, securing the bucket with IAM, and using IAM to grant permission to use Rekognition, we can open up a SageMaker instance and use Rekognition. Using a Jupyter notebook or a terminal instance in SageMaker, we can call Rekognition on our files in the bucket. We can make these calls using the AWS Command Line Interface or the boto3 client, which is the Python SDK for AWS. [Guide to the boto3 client here](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html).\n",
        "\n",
        "In this project, we used the boto3 client. Below is an example of one of out images, and the Rekognition call with the output for that. \n",
        "\n",
        "![Image](https://raw.githubusercontent.com/ethanj118/QuattroFormaggi_350Project/main/Raw%20Data/IMG_1043.jpg)\n",
        "\n",
        "![Code](https://raw.githubusercontent.com/ethanj118/QuattroFormaggi_350Project/main/Blog/Blog%20Images/Code%20screenshot.png)\n",
        "\n",
        "We can see that Rekognition is fairly confident that this person is calm, but the output also indicates that the model has some reason to believe that this person is confused. Still, the confidence is far greater for calm than confused. In our (human) prediction, we predicted that the person was calm with 75/100 confidence and happy with 25/100 confidence. In this case, our most confident prediction matched the one from Rekognition, but our second most confident prediction was different. \n",
        "\n",
        "**Data analysis**\n",
        "\n",
        "After getting results from Rekognition, we can use Python to build a dataframe and perform analysis on our results, either in a SageMaker notebook instance or using Google Colab. In Python, we can perform descriptive analyses of the data and run regressions for more advanced analysis. \n",
        "\n",
        "Before building the dataset, we needed to have a human prediction to compare to. One of our group members made his predictions for the emotion of the person in each picture, and assigned that prediction a confidence level from 1-100. If the chosen confidence level was less than 100, a second (or third) most confident prediction for the emotion was made until the total of all the confidence levels added up to 100. Although the Rekognition confidence levels were more precise than ours, this method yielded a similar type of data as the Rekognition output, allowing for comparisons and analyses to be made. To replicate this project, one could look at the raw data and make their own predictions for the emotion of each person."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD8bIoWqAl9K"
      },
      "source": [
        "## **Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r8i8F_ZApHD"
      },
      "source": [
        "## **Conclusions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaiUCn8gArn6"
      },
      "source": [
        "### **Articles Cited**\n",
        "\n",
        "\n",
        "\n",
        "Matsumoto, D., & Hwang, H. S. (2011, May). Reading facial \n",
        "  expressions of emotion. Psychological Science Agenda. http://\n",
        "  www.apa.org/science/about/psa/2011/05/facial-expressions \n",
        "\n",
        "Heaven, D. (2020, February 26). Why faces don't always tell the truth about feelings. Retrieved April 22, 2021, from https://www.nature.com/articles/d41586-020-00507-5\n"
      ]
    }
  ]
}
